[{"authors":["natbusa"],"categories":null,"content":"Senior Director of Data Engineering, Science, Analytics, Reporting, AI, and Big Data. Managing all data related activities \u0026amp; teams. Reporting directly to CFO/CEO.\nNatalino Busa is skilled at defining, designing and implementing custom big/fast data solutions for data-driven applications such as predictive analytics, personalized marketing, fraud detection and business event monitoring. Proficient in porting traditional ETL to Open Source data solutions.\nNatalino has successfully led teams across the world with effective, fit-for-purpose and reliable solutions. He provides a unique combination of business vision, applied data science and engineering skills.\n","date":1573460907,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1576948512,"objectID":"03b0e2502bc56b5054d2d7e8246a85ee","permalink":"/authors/natbusa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/natbusa/","section":"authors","summary":"Senior Director of Data Engineering, Science, Analytics, Reporting, AI, and Big Data. Managing all data related activities \u0026amp; teams. Reporting directly to CFO/CEO.\nNatalino Busa is skilled at defining, designing and implementing custom big/fast data solutions for data-driven applications such as predictive analytics, personalized marketing, fraud detection and business event monitoring. Proficient in porting traditional ETL to Open Source data solutions.\nNatalino has successfully led teams across the world with effective, fit-for-purpose and reliable solutions. He provides a unique combination of business vision, applied data science and engineering skills.","tags":null,"title":"Natalino Busa","type":"authors"},{"authors":["Natalino Busa"],"categories":["datafaucet"],"content":"Aggregations are an important step while processing dataframes and tabular data in general. And therefore, they should be as simple as possible to implement. Some notable data aggregation semantics are provided by pandas, spark and the SQL language.\nWhen designing an aggregation API method, the following characteristics make in my opinion a good aggregation method.\n easily perform aggregation on a column or a set of columns easily perform multiple aggregation functions on the same columns selectively perform differently aggregations on different columns  As an nice to have to this list, it would be nice to apply aggregation functions by passing the function name as a string. A good aggregation method should allow all the above with minimal amount of code required.\nGetting started Let's start spark using datafaucet.\nimport datafaucet as dfc  dfc.engine('spark')  \u0026lt;datafaucet.spark.engine.SparkEngine at 0x7fbdb66f2128\u0026gt;  spark = dfc.context()  Generating Data df = spark.range(100)  df = (df .cols.create('g').randint(0,3) .cols.create('n').randchoice(['Stacy', 'Sandra']) .cols.create('x').randint(0,100) .cols.create('y').randint(0,100) )  df.data.grid(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   id g n x y     0 0 1 Sandra 91 89   1 1 0 Sandra 19 57   2 2 2 Sandra 34 97   3 3 1 Stacy 35 15   4 4 2 Sandra 93 90     Pandas Let's start by looking how Pandas does aggregations. Pandas is quite flexible on the points noted above and uses hierachical indexes on both columns and rows to store the aggregation names and the groupby values. Here below a simple aggregation and a more complex one with groupby and multiple aggregation functions.\npf = df.data.collect()  pf[['n', 'x', 'y']].agg(['max'])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   n x y     max Stacy 97 98     agg = (pf[['g','n', 'x', 'y']] .groupby(['g', 'n']) .agg({ 'n': 'count', 'x': ['min', max], 'y':['min', 'max'] })) agg   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }  \n     n x y     count min max min max   g n          0 Sandra 9 14 75 3 98   Stacy 21 10 96 8 92   1 Sandra 20 8 91 9 91   Stacy 18 2 89 4 97   2 Sandra 12 4 97 1 98   Stacy 20 4 96 0 98     Stacking In pandas, you can stack the multiple column index and move it to a column, as below. The choice of stacking or not after aggregation depends on wht you want to do later with the data. Next to the extra index, stacking also explicitely code NaN / Nulls for evry aggregation which is not shared by each column (in case of dict of aggregation functions.\nagg = pf[['g', 'x', 'y']].groupby(['g']).agg(['min', 'max', 'mean']) agg = agg.stack(0) agg   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n    max mean min   g         0 x 96 50.966667 10   y 98 47.133333 3   1 x 91 45.026316 2   y 97 48.736842 4   2 x 97 58.750000 4   y 98 53.906250 0     Index as columns Index in pandas is not the same as column data, but you can easily move from one to the other, as shown below, by combine the name information of the various index levels with the values of each level.\nagg.index.names  FrozenList(['g', None])  agg.index.get_level_values(0)  Int64Index([0, 0, 1, 1, 2, 2], dtype='int64', name='g')  The following script will iterate through all the levels and create a column with the name of the original index level otherwise will use _\u0026lt;level#\u0026gt; if no name is available. Remember that pandas allows indexes to be nameless.\nlevels = agg.index.names for (name, lvl) in zip(levels, range(len(levels))): agg[name or f'_{lvl}'] = agg.index.get_level_values(lvl)  agg.reset_index(inplace=True, drop=True) agg   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   max mean min g _1     0 96 50.966667 10 0 x   1 98 47.133333 3 0 y   2 91 45.026316 2 1 x   3 97 48.736842 4 1 y   4 97 58.750000 4 2 x   5 98 53.906250 0 2 y     Spark (Python) Spark aggregation is a bit simpler, but definitely very flexible, so we can achieve the same result with a little more work in some cases. Here below a simple example and a more complex one, reproducing the same three cases as above.\ndf.select('n', 'x', 'y').agg({'n':'max', 'x':'max', 'y':'max'}).toPandas()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   max(x) max(y) max(n)     0 97 98 Stacy     Or with a little more work we can exactly reproduce the pandas case:\nfrom pyspark.sql import functions as F df.select('n', 'x', 'y').agg( F.lit('max').alias('_idx'), F.max('n').alias('n'), F.max('x').alias('x'), F.max('y').alias('y')).toPandas()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   _idx n x y     0 max Stacy 97 98     More complicated aggregation cannot be called by string and must be provided by functions. Here below a way to reproduce groupby aggregation as in the second pandas example:\n(df .select('g', 'n', 'x', 'y') .groupby('g', 'n') .agg( F.count('n').alias('n_count'), F.min('x').alias('x_min'), F.max('x').alias('x_max'), F.min('y').alias('y_min'), F.max('y').alias('y_max') ) ).toPandas()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   g n n_count x_min x_max y_min y_max     0 0 Sandra 10 17 96 8 98   1 0 Stacy 20 10 92 3 86   2 1 Stacy 18 4 89 4 97   3 2 Sandra 14 29 96 1 98   4 1 Sandra 20 2 91 4 97   5 2 Stacy 18 4 97 0 96     Stacking Stacking, as in pandas, can be used to expose the column name on a different index column, unfortunatel stack is currently available only in the SQL initerface and not very flexible as in the pandas counterpart (https://spark.apache.org/docs/2.3.0/api/sql/#stack)\nYou could use pyspark expr to call the SQL function as explained here (https://stackoverflow.com/questions/42465568/unpivot-in-spark-sql-pyspark). However, another way would be to union the various results as shown here below.\nagg = pf[['g', 'x', 'y']].groupby(['g']).agg(['min', 'max', 'mean']) a  from pyspark.sql import functions as F (df .select('g', 'x') .groupby('g') .agg( F.lit('x').alias('_idx'), F.min('x').alias('min'), F.max('x').alias('max'), F.mean('x').alias('mean') ) ).union( df .select('g', 'y') .groupby('g') .agg( F.lit('y').alias('_idx'), F.min('y').alias('min'), F.max('y').alias('max'), F.mean('y').alias('mean') ) ).toPandas()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   g _idx min max mean     0 1 x 2 91 45.026316   1 2 x 4 97 58.750000   2 0 x 10 96 50.966667   3 1 y 4 97 48.736842   4 2 y 0 98 53.906250   5 0 y 3 98 47.133333     Generatring aggregating code The code above looks complicated, but is very regular, hence we can generate it! What we need is a to a list of lists for the aggregation functions as shown here below:\ndfs = [] for c in ['x','y']: print(' '*2, f'col: {c}') aggs = [] for func in [F.min, F.max, F.mean]: f = func(c).alias(func.__name__) aggs.append(f) print(' '*4, f'func: {f}') dfs.append(df.select('g', c).groupby('g').agg(*aggs))   col: x func: Column\u0026lt;b'min(x) AS `min`'\u0026gt; func: Column\u0026lt;b'max(x) AS `max`'\u0026gt; func: Column\u0026lt;b'avg(x) AS `mean`'\u0026gt; col: y func: Column\u0026lt;b'min(y) AS `min`'\u0026gt; func: Column\u0026lt;b'max(y) AS `max`'\u0026gt; func: Column\u0026lt;b'avg(y) AS `mean`'\u0026gt;  The dataframes in this generator have all the same columns and can be reduced with union calls\nfrom functools import reduce reduce(lambda a,b: a.union(b), dfs).toPandas()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   g min max mean     0 1 2 91 45.026316   1 2 4 97 58.750000   2 0 10 96 50.966667   3 1 4 97 48.736842   4 2 0 98 53.906250   5 0 3 98 47.133333     Meet DataFaucet agg One of the goal of datafaucet is to simplify analytics, data wrangling and data discovery over a set of engine with an intuitive interface. So the sketched solution above is available, with a few extras. See below the examples\nThe code here below attempt to produce readable code, engine agnostic data aggregations. The aggregation api is always in the form:\ndf.cols.get(...).groupby(...).agg(...)\nAlternativaly, you can find instead of get\nd = df.cols.get('x').agg('distinct') d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   x     0 64     d = df.cols.get('x').agg(['distinct', 'avg']) d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   x_distinct x_avg     0 64 51.2     d = df.cols.get('x').agg(['distinct', 'avg'], stack=True) d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   _idx distinct avg     0 x 64 51.2     d = df.cols.get('x').agg(['distinct', 'avg'], stack='colname') d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   colname distinct avg     0 x 64 51.2     d = df.cols.get('x').agg(['distinct', F.min, F.max, 'avg']) d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   x_distinct x_min x_max x_avg     0 64 2 97 51.2     d = df.cols.get('x', 'y').agg(['distinct', F.min, F.max, 'avg']) d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   x_distinct x_min x_max x_avg y_distinct y_min y_max y_avg     0 64 2 97 51.2 67 0 98 49.91     d = df.cols.get('x', 'y').agg({ 'x':['distinct', F.min], 'y':['distinct', 'max']}) d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   x_distinct x_min x_max y_distinct y_min y_max     0 64 2 None 67 None 98     d = df.cols.get('x', 'y').agg({ 'x':['distinct', F.min], 'y':['distinct', 'max']}, stack=True) d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   _idx distinct min max     0 x 64 2.0 NaN   1 y 67 NaN 98.0     d = df.cols.get('x', 'y').groupby('g','n').agg({ 'x':['distinct', F.min], 'y':['distinct', 'max']}, stack=True) d.data.grid()  Extended list of aggregation An extended list of aggregation is available, both by name and by function in the datafaucet library\nfrom datafaucet.spark import aggregations as A d = df.cols.get('x', 'y').groupby('g','n').agg([ 'type', ('uniq', A.distinct), 'one', 'top3', ], stack=True) d.data.grid()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   g n _idx type uniq one top3     0 1 Stacy x int 23 67 {32: 2, 25: 2, 39: 2}   1 0 Stacy x int 16 74 {70: 1, 74: 1, 19: 1}   2 2 Sandra x int 10 40 {4: 2, 97: 2, 69: 1}   3 1 Sandra x int 13 52 {56: 1, 8: 1, 2: 1}   4 0 Sandra x int 13 79 {36: 2, 89: 1, 35: 1}   5 2 Stacy x int 20 45 {61: 1, 34: 2, 70: 1}   6 2 Stacy y int 13 98 {30: 2, 66: 2, 35: 2}   7 0 Stacy y int 13 57 {36: 1, 57: 1, 25: 1}   8 1 Sandra y int 16 79 {97: 2, 82: 2, 15: 3}   9 2 Sandra y int 14 40 {1: 1, 98: 1, 7: 1}   10 1 Stacy y int 17 76 {4: 2, 86: 2, 67: 1}   11 0 Sandra y int 15 24 {64: 1, 8: 1, 53: 2}      ","date":1573460907,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576948512,"objectID":"d065995801e2e789723be1df16a2a5e1","permalink":"/post/datafaucet-aggregate/","publishdate":"2019-11-11T16:28:27+08:00","relpermalink":"/post/datafaucet-aggregate/","section":"post","summary":"Aggregations are an important step while processing dataframes and tabular data in general. And therefore, they should be as simple as possible to implement. Some notable data aggregation semantics are provided by pandas, spark and the SQL language.\nWhen designing an aggregation API method, the following characteristics make in my opinion a good aggregation method.\n easily perform aggregation on a column or a set of columns easily perform multiple aggregation functions on the same columns selectively perform differently aggregations on different columns  As an nice to have to this list, it would be nice to apply aggregation functions by passing the function name as a string.","tags":["analytics","pyspark","pandas"],"title":"Aggregating Dataframes","type":"post"},{"authors":["Natalino Busa"],"categories":["architectures"],"content":"Many enterprises are moving from batch to streaming data processing. This engineering innovation provides great improvements to many enterprise data pipelines, both on the primary processes such as front-facing services and core operations, as well as on secondary processes such as chain monitoring and operational risk management.\nStreaming Analytics is the evolution of Big Data, where data throughput (velocity) and low-latency are important business KPIs. In such systems, data signals are ingested and produced at high speed - often in the range of millions of events per seconds. On top of that, the system has still to operate on large volumes of heterogeneous resources, it must execute complex processing to verify the completeness and accuracy of data. Finally, the produced output and data transformation must be produced fast enough to be relevant and actionable.\nBatch vs Streaming\nA batch chain is normally a series of transformations which happen sequentially, from source data to final results. Data moves one batch at a time from one step to the next one. Batch systems usually rely on schedulers to trigger the next step(s) in the pipeline, depending on the status of the previous step.\nThis approach suffers from a number of limitations:\n It usually introduces unnecessary latency from the moment the initial data is provided to the moment the results are produced. If those produced results were in fact insights, they might lose their \u0026ldquo;actionable\u0026rdquo; power because it is already too late to act. Responses and results are delivered after the facts, and the only analysis which can be done is a retrospective analysis, but it's too late to steer or correct the system, or to avoid the incidents in the pipeline. Decisions are made on results from aged or stale data, and they might be incorrect as the result do not reflect any longer the state of the system. This could produce over- and under- steering of the system. Data is at rest. This is not necessarily a drawback, but batch system tend to be passive, with time spent in extracting and loading data from file systems to databases and back with peaks and congestion on the enterprise network rather than a continuous flow of data.  A queue-centric approach to data processing\nFrom 10'000 feet high, a streaming analytics system can best be described as a queue. This logical, distributed queue connects agents producing data to those consuming data. Many components can functions both as sources and sinks for data streams. By highlighting the queue rather than the processing, we stress the fact that data is flowing, and data processing is always interleaved data transfer.\nThe same data element on the queue can potentially be required by many consumers. The pattern that best describe this is the publisher/subscriber pattern.\nAs the data transiting on the queue can be consumed at different rates, such a queue should also provide a certain persistence, acting as a buffer while producers and consumers are free to access data independently the one from the other.\nStreaming Analytics: Definition\nHere below a number of definitions which are widely accepted in the industry:\n\u0026ldquo;Continuous processing on unbounded tables\u0026rdquo; - Apache Flink, Data Artisans\n\u0026ldquo;Software that can filter, aggregate, enrich and analyze a high throughput of data from multiple disparate live data sources and in any data format to identify simple and complex patterns to visualize business in real-time, detect urgent situations, and automate immediate actions\u0026rdquo; - Forrester\nStreaming Analytics provides the following advantages w.r.t batch processing:  Events are analyzed and processed in real-time as they arrive Decisions are timely, contextual, and based on fresh data The latency from raw events to actionable insights in small Data is in motion and flows through the architecture  Furthermore, batch processing can be easily implemented on streaming computing architectures, by simply scanning the files or datasets. The opposite is not always possible, because the latency and processing overhead of batch systems is usually not negligible when handling small batches or single events.\nStreaming Analytics: Events Streams vs Micro-Batches\nNext to latency and throughput, another important parameter which defines a streaming analytics system is the granularity of processing. If the system handles streams one event at a time, we define it as an event-based streaming architecture, if the streams gets consumed in packets/groups of events we call it a micro-batching streaming architecture. In fact you could consider a batch pipeline a streaming architecture, albeit a very slow one, handling the streaming data in very large chunks!\nThe following two pictures give an intuition of how those two paradigms work:\nWhy Streaming Analytics for Chain Monitoring\nEnterprise BI processing chains tend to be very complex, because of the volume, but also because of the number of regulations and compliance measures taken. Hence it's not uncommon that process changes and unforeseen load can strain part of the chain, with oftentimes big consequences. When incidents occur several steps if not of the entire chain must be re-run. These incidents are often a source of delays, reduced service level and in general lower quality of internal BI process measures and KPIs.\nStreaming Analytics can be effectively used as the processing paradigm to control and act on metadata produced by BI chains:\n  Build models using a large amount of sensor meta-data, events, and facts, and determine which patterns are normal and which are anomalous in the received data\n  Score, forecast and predict trends on newly generated data, and provide real-time actionable insights\n  Use ETL logs and meta-data to forecast data quality and process operational kpi's\n Forecasting for Time, Volume, Query Types Forecasting on Errors and Incidents  Rationale:\nData values tend to be stable around some statistics, therefore we could collect the logs and build a model complying on the statistics of incidents, and other monitors values in order to determine the chance of success of a given ETL pipeline. Same sort of analysis can be applied to variables such as ETL jobs logs to monitor and process volumes, time of processing, query types . This information can be captured at run-time as the ETL jobs are executing. Here below a few examples of anomaly predictions and time series forecasting on machine logs.\nObjectives:\n Detect anomalies in log data variables Predict behaviour of ETL processes Predict the probability of future incidents  ref: https://github.com/twitter/AnomalyDetection\nUse ETL logs and meta-data to identify records and data anomalies\n Detect missing/duplicated data Anomaly detection on patterns and queries types   Rationale:\nData values tend to be stable around some statistics, therefore we could use this statistics to characterize future data and detect potential incident early on the ETL process.\nThis analysis exploit the nature of data being processed as well as the metadata provided by the ETL tools themselves, to increase the chances of both prediction and detection.\nObjectives:\n Monitor the records of specific products or data categories Cluster and group Data Logs specific to given categories and collections Detect Anomalies based on Volume, Query types, error count, log count, time, etc  Use ETL logs and meta-data to cluster and classify queries and data transformations\nCluster processing based on queries types, query result statuses, access logs, and provide an indication on the \u0026ldquo;norms\u0026rdquo; for data and process quality as well as detect possible intrusions and cyber security attacks.\nRationale:\nETL metadata, is a rich source of information. Normally this information is manually curated. However metadata is data. And as such it can be processed as text, text extraction techniques can be applied to db logs, query logs and access logs.\nOnce the data is being structured, machine learning and data science techniques can be applied to detect clusters, and (semi) automatically classifying datasets, providing higher SLA, better data quality, and higher prevention of both incidents as well as cybersec attacks.\nObjectives\n Extract patterns and information from machine logs Combine multiple sources Normalize the data into a single format Apply machine learning algorithms to cluster and classify the given information  Data Governance on Streaming Data\nStreaming data is still data. Hence, it must be managed and governed. One way of managing data is by logically partitioning it in semantic layers, from raw data sources to actionable output. In particular, Streaming data can also be layered: from raw events to alerts and notifications.\nStreaming Data Components:\nAs depicted in the diagram on the right, a streaming analytics, can be logically split three logical function classes:\n Data Capture Data Exploration Data Exploitation  This can be mapped on 6 logical components:\n Data Extraction Data Transport Data Storage Data Analytics Data Visualization Data Signaling  Selection of Streaming Analytics Components\nIf we consider the streaming components as a stack, we can select for each component a number of tools available in the market. Therefore, we can define a number of bundles or recipes depending on the technology used for each component of the stack. In the diagram below you can see a fee of those streaming analytics bundles.\nSome of those bundles are composed of open source projects, others by proprietary closed-source technologies. This first classification positions technologies such as Splunk, HP, and Teradata, SQLStream in one group and the SMACK, ELK, Flink stacks in another. Moreover, some bundles are fully delivered and maintained by a single company (Splunk, HP Arcsight, Elastic) while others bundles are composed by tools maintained by different companies and dev centers (Teradata, Flink, SMACK).\nAlso, considering the streaming analytics use cases, some of this bundles are better tuned to specific domains (cyber security, marketing, operational excellence, infrastructural monitoring) while others are more less specialized and can be tuned or customized to a specific set of use cases.\nWhile the following diagram is not exhaustive, it provides a selection of some of the most bespoken and widely adopted components from streaming analytics as available today in the market.\nScorecard\nThe following scorecard can be used to determine which individual components and which bundles are more appropriate and fit-for-purpose provided the use cases, the organization, the capabilities both in terms of people, tools, and technology, the business and financial goals and constraints, and the culture of the given enterprise.\n Metrics, Criteria Rationale Open Source  Sharing the source code, provides a higher level of transparency.   Ease of Use  How easy it is to implement new use cases? Or to modify existing ones?   Vendor Specific  Some components, once used might be hard to swap for others\nbecause of the level of tuning and customization and create technologies lock-ins.   Documentation  Is the tool well documented? What about, Install, configuration, and examples?   Community  An active community stimulates and steer the innovation\nprocess and provides feedback on features, bugs and best practices.   Easy of IT Integration  How straightforward it is to provide this   Longevity  The amount of year of the a given technology in the\nmarket provides an indication of the maturity of the solution.   Libraries  Are Plugins and 3rd Party Libraries available? Is there a marketplace, and a community of satellite companies\ncontributing to the technology?   Maintenance  SLA may vary depending of the use case and other requirements   Performance  How fast are streams processed? How efficient is the solution provided the same amount of IT resources?   Release cycle  How often are new releases delivered?   TCO  What is the estimated total cost of ownership for the selected cpmponents?   Data Integration  Can the available data sources be directly used? What about data models and formats?   Expertise  Are experts available in the job market? Can they be easily acquired?   Data Volumes  How well can the selected technology cope with the data volumes generated?   Learning Curve  How much time does it take to master this technology\nfrom a user/dev/ops perspective?   Data Aggregation  When models require large context, how well can\nthe technology join and merge data?   User and Access Management  How well does this solution fit to the security and auditing measures setup in the enterprise?    Streaming Meta-Data: Monitoring BI chains\nFrom a logical architecture perspective, streaming analytics processing can be seen as data transformations or computing step which fetch data from a distributed queue and push results back to the queue, as previously explained on the log-centric conceptual diagram of streaming computing.\nIn the previous diagram the logical functions of a streaming analytics systems are divided in groups, depending on the nature of the processing. You could govern streaming analytical functions according to the following taxonomy:\n Capturing  Object Store File Store   Data Logging  Data Acquisition via APIs Data Listeners (files, sockets) Data Agents (browsers, devices)   Transformation  Data Cleaning Data Augmentation Data Filtering Data Standardization Sessioning, Grouping Data Formatting Data Compaction   Modeling  Data Modeling Clustering Pattern Extraction Feature Engineering Histogram Analysis Norms Extraction Machine Learning / AI Anomaly Detection Forecasting Recommendation Classification   Signaling  Alerting Notification    Streaming Analytics: Conceptual Architecture\nBefore diving in the detailed in the architectural blueprint, let us analyze the main components of such a system. The diagram here below provides a simplified description of the different parts constituting a streaming analytics architecture.\nStarting from the bottom, we define two storage layers, the top two layers are analytics, and visualization.\nThe first is a typical a Big Data layer for long term storage of data. It provides an excellent and cost efficient solution to store raw stream events and meta-data. Data on this layer is most efficiently stored in large files. This layer is usually not great for random access of specific records, but works well to stream out large files and have them processed in engines such as Presto, Hive, and Spark.\nThe second storage layer is more tailored toward objects and documents. The characteristic of this layer is that access is fast. This form of storage provides better data search and exploration functions. Moreover, a document store provides fast searches by indexing textual data, and fast access to individual stream events/elements. This layer is typically realized using NoSQL technologies, out of which two of them Cassandra, and Elasticsearch, are discussed in better details in the following sections.\nThe third layer is meant for model building and data exploration. Presto and Hive are SQL engines part of the Hadoop ecosystem and they are tuned respectively for interactive exploratory queries and large batch analysis on big data volumes. Spark is also an interesting components as it allows to interleave Machine Learning operations with both SQL queries and data transformations using languages such as Scala and Python.\nThe top layer is populated by data visualization tools. These tools usually access the underlying analytical layer in order to perform the computations, and then display the results using dashboards, graphs and widgets, often via a Web UX.\nStreaming Analytics: Architectural Blueprint and Data Landscape\nThe following architectural blueprint, provides a possible implementation for meta-data managements and chain monitoring. It consists of three main parts.\nDescription:\nThis open source blueprint serves a number of goals:\n long term storage of the raw events (data lake) Data exploration and validation of models and hypotheses Implementation and development of ad-hoc use cases Model creation and model validation using data science and machine learning tools.  Considerations\nThe above blueprint architecture is a possible end state for chain monitoring and operational excellence. It can definitely be phased in stages according to the organization's appetite, roadmap and strategy to streaming analytics and real-time data processing.\nOne general remark is that each streaming technology and each component of the above blueprint has its \u0026ldquo;sweet spot\u0026rdquo; in the overall data landscape.\nElasticsearch is extremely efficient at storing, capturing and display time series data. However because of the way the data is structured complex queries and joins are usually not performed efficiently within this platform. This is way for complex query Elasticsearch can be complemented by other solutions such as Spark, Presto, Hive, Cassandra or other analytical systems such as enterprise data warehouses to act as \u0026ldquo;powerhouse\u0026rdquo; for complex queries and aggregation.\nSee diagram here below:\nThe proposed combination of file and object data stores, topped by Spark is quite powerful and provided probably the highest level of flexibility in order to implement each specific use case, in a tailored and customized way. Spark uniqueness comes from the fact that it provides a unified data programming paradigm. Spark combines SQL, Python, Scala, Java, R, as well as streaming and machine learning capabilities under the same programming paradigm, and using the very same engine to perform this variety of computations.\nRecommendations\nThe suggested blueprint requires of course further analysis and it's advised to determine which scoring criteria should weigh more in the selection and determine which components or bundles in the architecture should be prioritized.\nIt's also probably wise, seen the vast choice of components, tools, libraries and solutions to identify which level of integration (libraries or ready made packaged solutions) is preferred in the organization. Depending on the availability of devops resources, you can trade flexibility, and customized solution for pre-canned use-case specific solutions.\nActive human-manned monitoring is becoming unfeasible, especially when hundreds of dashboards are produced by systems such as Kibana. It's therefore highly recommended to complement the dashboarding approach to a more data-driven solution where patterns and anomalies are learned and detected autonomously by the system.\nAlso the availability of raw metadata signals as part of this architecture stored in a data lake and transported on kafka will probably constitute a great substrate to create and develop other use case in other domains (fraud, cybersecurity, marketing, personalized recommenders, predictive services etc.)\nStreaming Analytics Engines: Open Source Projects\nFor further reading, let's focus on computing engines, as streaming computing engines are the foundation for any domain-specific streaming application. Here below, it's presented a selection of streaming processing technologies which have been developed in the last years:\nFor a detailed description of those technologies, have a look at this post:\nhttps://www.linkedin.com/pulse/streaming-analytics-story-many-tales-natalino-busa\n","date":1486456107,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576925784,"objectID":"0be4a221ab0f46645852e7241bfd66b0","permalink":"/post/streaming-analytics/","publishdate":"2017-02-07T16:28:27+08:00","relpermalink":"/post/streaming-analytics/","section":"post","summary":"Many enterprises are moving from batch to streaming data processing. This engineering innovation provides great improvements to many enterprise data pipelines, both on the primary processes such as front-facing services and core operations, as well as on secondary processes such as chain monitoring and operational risk management.\nStreaming Analytics is the evolution of Big Data, where data throughput (velocity) and low-latency are important business KPIs. In such systems, data signals are ingested and produced at high speed - often in the range of millions of events per seconds. On top of that, the system has still to operate on large volumes of heterogeneous resources, it must execute complex processing to verify the completeness and accuracy of data.","tags":["analytics","streaming"],"title":"Streaming Analytics","type":"post"},{"authors":["Natalino Busa"],"categories":["meetups"],"content":"A few weeks back I was lucky enough to attend and present at the Global AI Summit in the bay area. This is my personal trip report about the people I have met and some of the projects and startups I came across.\nAI is eating the world.\nFirst of all, let me start by saying that literally everybody is doing (or claiming to do) AI in the bay area. AI has inflamed the spirits of pretty much every single software engineer, data scientist, business developer, talent scout, and VC in the greater San Francisco area.\nAll tools and services presented at the conference embed some form of machine intelligence, and scientists are the new cool kids on the block. Software engineering has probably reached an all-time low in terms of coolness in the bay area, and regarded almost as the \u0026ldquo;necessary evil\u0026rdquo; in order to unleash the next AI interface. This is somewhat counter-intuitive, as actually Machine Learning and AI are more like the raisins in raisin bread, as Peter Norvig and Marcos Sponton say.\nWhat's behind AI?\nGood engineering, and great business focus is still the foundation of many AI-powered tools and services. In my opinion, there is no silver bullet: AI powered applications must still be based on good engineering practices if they want to succeed.\nWe have seen a similar wave during the dot-com bubble in the beginning of the millennium when web-shops were popping up with little understanding of the underlying retail and marketplace businesses. Since then, web applications have matured and today we value those services both for their digital journey as much as for their operational excellence and their ability to deliver. I believe that a similar maturing path will happen for AI powered applications.\nAI is still a very opaque concept. In the worst case it could be just a scripted process, more often is a set of predictive machine learning models. Because of the vagueness of the term, others are branding new terms in order to differentiate themselves: machine intelligence, cognitive/sentient computing, intelligent computing. Advertising more AI-related terms is not really helping clarifying what is running under the hood. After some digging, startups operating in the AI space are mainly interpreting AI as some form of artificial/machine learning (aka weak AI) tailored to very specific tasks.\nToday, with some exceptions, the term AI is used to describe Artificial Neural Networks (ANNs), and Deep Learning (DL) mostly related to text, speech, image, and video processing. Putting the hype aside for a moment, without any doubts we can acknowledge that the renaissance of deep learning has contributed to the development of conversational interfaces.\nThe core of this new generation services might by still hard-coded or scripted, but the interface is going to be more and more flexible, understanding our spoken, text, and visual cues. This human-centric approach to UIs is definitely going to shape the way we interact with devices. This trend goes under the buzz of Natural/Zero UIs.\nLet's go deeper in the stack, away from the front-end and human-machine Natural UIs. Narrow AI, in particular deep learning and hierarchical predictive models are getting traction as core data components, in particular for applications such as recommender systems, fraud detection, churn and propensity models, anomaly detection and data auditing.\nBefore moving on the following list: I am not associated with any of these companies, however I did find their approach worth mentioning and good food for thoughts for the entrepreneurs and the data people following this blog. So, as always, take the following with a pinch of salt and apply your critical \u0026amp; analytical thinking to it. Enjoy :)\nNumenta is tackling one of the most important scientific challenges of all time: reverse engineering the neocortex. Studying how the brain works helps us understand the principles of intelligence and build machines that work on the same principles. They have invested heavily in time series research, anomaly detection and natural language processing. By converting text, and geo-spatial data to time series Numenta can detect patterns and anomalies in temporal data.\nRecognos\u0026rsquo; main product \u0026ldquo;Smart Data Platform\u0026rdquo; is meant to normalize and integrate the data that is stored in unstructured, semi-structured and structured content. The data unification process is driven by the business ontology. Data extraction, taxonomy, semantic tagging and structured data mapping are all steps in this modern approach to data preparation and normalization. Recognos\u0026rsquo; ultimate goal is to allow to use the data that is stored in the un-structured, semi-structured and structured content using a unique semantic meaning and unique query language.\nAppzen is picking up the challenge of automating the auditing of expenses in real-time. This service collects all receipts, tickets, and other documentation provided and produces a full understanding of the who, where, and why of every expense. Appzen\u0026lsquo;s machine learning engine verifies receipts, eliminates duplicates, and searches through hundreds of data sources to verify merchant identities and validate the truthfulness of every expense – ensuring there is no fraud, misuse, or violation of laws.\nTalla is a lightweight system which plugs into messaging systems as a virtual team members, executing on-boarding, e-learning, and team building process flows, and engaging with the various team members, taking over from tasks which are usually done by team managers, scrum masters, and facilitating teams. It employs a combination of natural language processing, robotic process automation, and user- and company- defined rules.\nInbenta has build an extensive multilingual knowledge supporting over 25 native languages and counting, including English, Spanish, Italian, Dutch and German. Its NLP engine understands the nuances of human conversation, so it answers a query based on meaning, not individual keywords. This is a good example of a company which relies on good business development and a great core team of linguistics and language experts. By combining these elements with NLP and Deep Learning techniques they can power chatbots, email management, surveys and other text-based use cases for a number of verticals.\nLymba is also tackling textual information, with the goal of extracting insights and non-trivial bits of knowledge from a semantic graph of heterogeneous linked data elements. Lymba offers transformative capabilities that enable discovery of actionable knowledge from structured and unstructured data, providing business with invaluable insights. Lymba has developed a technology for deep semantic processing that extracts semantic relations between all concepts in the text. One of Lymba's product, the \u0026ldquo;K-extractor\u0026rdquo; enables intelligent search, question answering, summarization of a document, generating scientific profiles, etc.\nJetlore is bringing personalization one step further by creating websites and mobile apps which are extremely tailored to each user, both in terms of content as well as layout, color, highlights, promotions, images, and offers. All site assets are ranked individually for each customer, and selected at the time of interaction based on the layout's configuration. Jetlore can select the best categories, brands, and collections of products from your inventory for each user, and automatically feature the best images to represent them. Ownerlisten is a smart messaging pipelining solution, rerouting messages in a given organization to the right person or process depending on the nature of the message. Users can filter and process messages combining business rules as well as automated text processing. This is essential in businesses where machine learned models might not be provide sufficiently accuracy for certain topics. Ownerlisten is another good example of how AI and NLP can be organically combined with user defined messaging and communication flows. By combining domain expertise, solid engineering and NLP engines, ownerlisten can deliver very smooth user and customer journeys in a number of different industries and use cases.\nThis list would not be complete without at least a company offering AI, Machine Learning, Data plumbing, Data engineering, API and Application engineering services. Software and data engineering, might be less cool a land of scientists, but it's still the backbone on top of which all those awesome solutions and products are built. Data Monster is one of those great studios accelerating mvp and product development, with a strong affinity to data processing at scale and all the right techs in the basket (Scala, Python, R, Java, JavaScript, Hadoop, Spark, Hive, Play, Akka, MySQL, PostgreSQL, AWS, Cassandra, etc ).\nI finish this post mentioning and thanking a number of great people I have met during this trip, for their charisma and inspiring ideas and conversations: Hamid Pirahesh, David Talby, Alexy Khabrov, Alexander Tsyplikhin, Christopher Moody, Michael Feng, Delip Rao, Eldar Sadikov, Michelle Casbon, Mustafa Eisa, Ahmed Bakhaty, Adi Bittan, Jordi Torras, and Francesco Corea.\n","date":1486456107,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576925784,"objectID":"1a5d73704dcfd5b490678eb64b9734d5","permalink":"/post/ai-trip/","publishdate":"2017-02-07T16:28:27+08:00","relpermalink":"/post/ai-trip/","section":"post","summary":"A few weeks back I was lucky enough to attend and present at the Global AI Summit in the bay area. This is my personal trip report about the people I have met and some of the projects and startups I came across.\nAI is eating the world.\nFirst of all, let me start by saying that literally everybody is doing (or claiming to do) AI in the bay area. AI has inflamed the spirits of pretty much every single software engineer, data scientist, business developer, talent scout, and VC in the greater San Francisco area.\nAll tools and services presented at the conference embed some form of machine intelligence, and scientists are the new cool kids on the block.","tags":["analytics","ventures"],"title":"The AI scene in the valley","type":"post"},{"authors":["Natalino Busa"],"categories":["meetups"],"content":"I was kindly asked by Prof. Roberto Zicari to answer a few questions on Data Science and Big Data for www.odbms.org - Let me know what you think of it, looking forward to your feedback in the comment below. Cheers, Natalino\nQ1. Is domain knowledge necessary for a data scientist?\nIt’s not strictly necessary, but it does not harm either. You can produce accurate models without having to understand the domain. However, some domain knowledge will speed up the process of selecting relevant features and will provide a better context for knowledge discovery in the available datasets.\nQ2. What should every data scientist know about machine learning?\nFirst of all, the foundation: statistics, algebra and calculus. Vector, matrix and tensor math is absolutely a must. Let’s not forget that datasets after all can be handled as large matrices! Moving on specifically on the topic of machine learning: a good understanding of the role of bias and variance for predictive models. Understanding the reasons for models and parameters regularization. Model cross-validation techniques. Data bootstrapping and bagging. Also I believe that cost based, gradient iterative optimization methods are a must, as they implement the “learning” for four very powerful classes of machine learning algorithms: glm’s, boosted trees, svm and kernel methods, neural networks. Last but not least an introduction to Bayesian statistics as many\nQ3. What are the most effective machine learning algorithms?\nRegularized Generalized Linear Models, and their further generalization as Artificial Neural Networks (ANN’s), Boosted and Random Forests. Also I am very interested in dimensionality reduction and unsupervised machine learning algorithms, such as T-SNE, OPTICS, and TDA.\nQ4. What is your experience with data blending?\nBlending data from different domains and sources might increases the explanatory power of the model. However, it’s not always easy to determine beforehand if this data will improve the models. Data blending provide more features and the may or may be not correlated with what you wish to predict. It’s therefore very important to carefully validate the trained model using cross validation and other statistical methods such as variances analysis on the augmented dataset.\nQ5. Predictive Modeling: How can you perform accurate feature engineering/extraction?\nLet’s tackle feature extraction and feature engineering separately. Extraction can be as simple as getting a number of fields from a database table, and as complicated as extracting information from a scanned paper document using OCR and image processing techniques. Feature extraction can easily be the hardest task in a given data science engagement.\nExtracting the right features and raw data fields usually requires a good understanding of the organization, the processes and the physical/digital data building blocks deployed in a given enterprise. It’s a task which should never be underestimated as usually the predictive model is just as good as the data which is used to train it.\nAfter extraction, there comes feature engineering. This step consists of a number of data transformations, oftentimes dictated by a combination of intuition, data exploration, and domain knowledge. Engineered features are usually added to the original samples’ features and provided as the input data to the model.\nBefore the renaissance of neural networks and hierarchical machine learning, feature engineering was as the models were too shallow to properly transform the input data in the model itself. For instance, decision trees can only split data areas along the features’ axes, therefore to correctly classify donut shaped classes you will need feature engineering to transform the space to polar coordinates.\nIn the past years, however, models usually have multiple layers, as machine learning experts are deploying increasingly “deeper” models. Those models usually can “embed” feature engineering as part of the internal state representation of data, rendering manual feature engineering less relevant. For some examples applied to text check the section “Visualizing the predictions and the “neuron” firings in the RNN” in The Unreasonable Effectiveness of Recurrent Neural Networks. These models are also usually referred as “end-to-end” learning, although this definition it’s still vague not unanimously accepted in the AI and Data Science communities.\nSo what about feature engineering today? Personally, I do believe that some feature engineering is still relevant to build good predictive systems, but should not be overdone, as many features can be now learned by the model itself, especially in the audio, video, text, speech domains.\nQ6. Can data ingestion be automated?\nYes. But beware of metadata management. In particular, I am a big supporter of “closed loop” analytics on metadata, where changes in the data source formats or semantics are detected by means of analytics and machine learning on the metadata itself.\nQ7. How do you ensure data quality?\nI tend to rely on the “wisdom of the crowd” by implementing similar analyses using multiple techniques and machine learning algorithms. When the results diverge, I compare the methods to gain any insight about the quality of both data as well as models. This approach works also well to validate the quality of streaming analytics: in this case the batch historical data can be used to double check the result in streaming mode, providing, for instance, end-of-day or end-of-month reporting for data correction and reconciliation.\nQ8. What techniques can you use to leverage interesting metadata?\nFingerprinting is definitely an interesting field for metadata generation. I have worked extensively in the past on audio and video fingerprinting. However this technique is very general and can be applied to any sort of data: structure data, time series, etc. Data fingerprinting can be used to summarize web pages retrieved by users or to define the nature and the characteristics of data flows in network traffic. I also work often with time (event time, stream time, capture time), network data (ip/mac addresses, payloads, etc.) and geolocated information to produce rich metadata for my data science projects and tutorials.\nQ9. How do you evaluate if the insight you obtain from data analytics is \u0026ldquo;correct\u0026rdquo; or \u0026ldquo;good\u0026rdquo; or \u0026ldquo;relevant\u0026rdquo; to the problem domain?\nInitially, I interact with domain experts for a first review on the results. Subsequently, I make sure than the model is brought into “action”. Relevant insight, in my opinion, can always be assessed by measuring their positive impact on the overall application. If human interaction is in the loop, the easiest method is actually to measure the impact of the relevant insight in their digital journey.\nQ10. What were your most successful data projects? And why?\n1. Geolocated data pattern analysis, because of its application to fraud prevention and personalized recommendations. 2. time series analytics for anomaly detection and forecasting of temporal signals - in particular for enterprise processes and KPI’s. 3. Converting images to features, because it allows images/videos to be indexed and classified using standard BI tools.\nQ11. What are the typical mistakes done when analyzing data for a large scale data project? Can they be avoided in practice?\nAggregating too much will most of the time “flatten” signals in large datasets. To prevent this, try using more features, and/or provide a finer segmentation of the data space. Another common problem is “buring” signals provided by a small class of samples with those of a dominating class. Models discriminating unbalanced classes tend to perform worse as the dataset grows. To solve this problem try to rebalance the classes by applying stratified resampling, or weighting the results, or boosting on the weak signals.\nQ12. What are the ethical issues that a data scientist must always consider?\n1. Respect individual privacy and possibly enforce it algorithmically. 2. Be transparent and fair on the use of the provided data with the legal entities and the individuals who have generated the data. 3. Avoid building models discriminating and scoring based on race, religion, sex, age etc as much as possible and be aware of the implication of reinforcing decisions based on the available data labels.\nOn last point, I would like to close this interview with an interesting idea around “equal opportunity” for ethical machine learning. This concept is visually explained on the following Google Research page Attacking discrimination with smarter machine learning from a recent paper by Hardt, Price, Srebro.\n","date":1486425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576925784,"objectID":"a1b90f4ebe30cf180856349d236c92e9","permalink":"/post/data-science-qa/","publishdate":"2017-02-07T00:00:00Z","relpermalink":"/post/data-science-qa/","section":"post","summary":"I was kindly asked by Prof. Roberto Zicari to answer a few questions on Data Science and Big Data for www.odbms.org - Let me know what you think of it, looking forward to your feedback in the comment below. Cheers, Natalino\nQ1. Is domain knowledge necessary for a data scientist?\nIt’s not strictly necessary, but it does not harm either. You can produce accurate models without having to understand the domain. However, some domain knowledge will speed up the process of selecting relevant features and will provide a better context for knowledge discovery in the available datasets.\nQ2. What should every data scientist know about machine learning?","tags":["analytics","data science"],"title":"Data Science Q\u0026A","type":"post"},{"authors":["Natalino Busa"],"categories":["rewind"],"content":"2016 has been simply incredible. What you will read next is a summary of my journey last year. It's mostly an open letter to thank all the wonderful people that inspired and supported me in 2016.\nJob wise, the year started at ING as enterprise data architect where I followed closely several scrum teams on cutting edge projects such as streaming analytics, fraud and cyber security, api's, data lakes, and predictive analytics. I really would like to thank ING, in particular Henk Kolk, Peter Jacobs, Beate Zwijnenberg, Paul Jacobs, and Ferd Scheepers and so many others which I have met during my last engagements at ING.\nMy journey has continued in 2016 with Teradata, where I am currently involved in the middle of a huge transformation from products to services, where Teradata is expanding from an EDW vendor to a data-driven solution selling enterprise, where open source, data science, and tailored data architecture are becoming the new \u0026ldquo;bread and butter\u0026rdquo; for new business creation. A \u0026ldquo;thank you\u0026rdquo; is due to the Teradata team in particular to Sergio Franssen, Head of Professional Services in Benelux and Eric-Jan van Meningen, Global Sales Director and to a great team of awesome data scientists and data engineers.\nAlong the way, I have enjoyed participating to a number of initiatives: writing blogs and tutorials for the O'Reilly Media Group where I would like to thank Ben Lorica, Paco Nathan, Shannon Cutt, Jenn Webb and so many others for the warm support during the last projects together. Deepening into AI with the Cognitive Finance Group founded by Clara Durondie. Also supporting the Palladium team, thanks to Mohamed El Aref. Last but not least, I have always enjoyed the meetups of the IBM Spark Technology Council, and I would like to thank in particular Robert Thomas, Katharine Kearnan, Remus Lazar and Holden Karau for the great discussion about Spark devs at the IBM spark technology center and pokemons.\nTech evangelism would not be worth this name without a thriving community of crazy coders, mad scientists, and puny hackers, and the many online and life meetup sessions and code sharing events, webinars and hot discussions on the future of Data Computing and Data Science, thank you all for sharing code and ideas! I have learned so much from you all.\nSome buddies of mine out there are building some amazing techs so I wish the very best to Edward Kibardin (datarefiner.com), Chris Fregly (pipeline.io), Olaf Molenveld (vamp.io), Gaspare Maria (continua) for keeping up the great work of building AI and Data-Driven solutions on top of bespoken open-source technologies.\nInspiration is a two way street, as the energy and the effort you put into studying, learning and developing ideas comes back as feedback and support from those who follow you. So a big thank you, to you who are reading this right now. It's you that push me to do event better the next time and keep it up the great work.\nTechnology: What have I learned this year.\n2016 has seen me getting even more involved with Spark, both on the Scala and the Python APIs. Next to that, loads of deep learning on my daily diet, in particular Keras and Tensorflow, both on bespoken data sets (CIFAR, MNIST) but also \u0026ldquo;on the job\u0026rdquo; on financial, telecom and social data. Another fantastic discovery this year was the Jupyter project as it goes way way further than the bespoken notebook. In particular, I have spent some time on jupyter-powered APIs with the jupiter gateway project. Also notable mention to the Apache Toree project which provides a Spark/Scala kernel to Jupyter, as I consider it probably the best notebook integration available, next to the Spark Notebook of my buddies Andy Petrella and Xavier Tordoir at kensu.io\nTechnology wise, this year was the definitely the year of Spark, Flink, Cassandra, ElasticSearch, Akka and Kafka. A lot of great architectures and stacks have be crafted using these building blocks for a variety of use cases and scenarios, such as Finance, Telecom, and HealthTech.\nCloud technologies have also been a revelation for me in 2016, with advances and innovations such as containers-as-a-service, cloud AI/ML services, better virtualization technologies with GPU support and better resource negotiators. Worth mentioning three techs I have enjoyed this year are Docker, Mesos and Kubernetes.\nI kept the course on the following, going deeper understanding the techs:\n Spark Cassandra Elasticsearch Kafka Akka Scala Python Scikit-Learn Pandas  New learning items this year:\n Keras Tensorflow Jupyter Toree Docker  Public Speaking\nIt was an incredible year for speaking, and i was honoured to present at some really great events on some of my lifetime passions/topics:\n Data Science Big Data Distributed Computing Cloud Computing Streaming Analytics AI and ML Predictive APIs  Talks, Meetups, Presentations:\nIn total I delivered the following during this year:\n  8 webinars at Bright Talks\n https://www.brighttalk.com/search?duration=0..\u0026amp;q=busa\u0026amp;rank=entrytime    8 live talks\n PAPIs.io, Eurocloud, Strata London, AI in Finance London, Big Data Spain, Teradata Innovation Forum Rotterdam/Helsinky, Big Data Analytics Amsterdam    1 podcasts\n Understanding predictive analytics    4 editorials\n How to build an anomaly detection engine with Spark, Akka, and Cassandra Clustering geolocated data using Spark and DBSCAN The evolution of advanced analytics in Spark Why is the SMACK stack all the rage lately?    6 video's\n https://www.youtube.com/results?sp=CAI%253D\u0026amp;q=natalino+busa https://www.youtube.com/watch?v=Mm0g39R9xVU https://www.youtube.com/watch?v=0j2XfaHqRQ0 https://www.youtube.com/watch?v=w1WRk-RAkp0 https://www.youtube.com/watch?v=IAknGMedg6U https://www.youtube.com/watch?v=WLIbMqFZmHU    3 blog posts on LinkedIn Pulse\n The Data Science Singularity Containers as a Service: Swarm vs Kubernetes vs Mesos vs Fleet vs Yarn Analytics in the Cloud    483 tech updates on linkedin and twitter\n  4984 followers on linkedin\n  Some projects and engagements:\n Big Data and Data Science Strategy in Banking, KSA Streaming Analytics Consulting in Banking, The Netherlands Big Data Consulting in Banking, The Netherlands AI for Banking Services in London, UK  Looking Forward in 2017:\nThe year to come is the year where Big Data, Streaming Analytics, Data Science and AI/ML will get traction and most likely be adopted by most companies in one way or another. I will be happy to provide consultancy on architectures, tools and algorithms for this new wave of data-driven products. The future is bright, I wish you all a great 2017 and the very best for you next projects!\nClosing on, I would like to thank my friends Carlo Canova, Antonios Zagaris, Ghiath Alkadi, Giuseppe Grillo, Enith Vlooswijk, my lovely kids Ryan, Nemo and Maya, my family and friends, Have a great 2017, everybody!\n","date":1482969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576925784,"objectID":"8643746f6f9b46e4851d084308baf02b","permalink":"/post/rewind-2016/","publishdate":"2016-12-29T00:00:00Z","relpermalink":"/post/rewind-2016/","section":"post","summary":"2016 has been simply incredible. What you will read next is a summary of my journey last year. It's mostly an open letter to thank all the wonderful people that inspired and supported me in 2016.\nJob wise, the year started at ING as enterprise data architect where I followed closely several scrum teams on cutting edge projects such as streaming analytics, fraud and cyber security, api's, data lakes, and predictive analytics. I really would like to thank ING, in particular Henk Kolk, Peter Jacobs, Beate Zwijnenberg, Paul Jacobs, and Ferd Scheepers and so many others which I have met during my last engagements at ING.","tags":["analytics","ventures","projects"],"title":"Rewind 2016","type":"post"}]