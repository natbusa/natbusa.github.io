<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>meetups | Natalino Busa</title>
    <link>/categories/meetups/</link>
      <atom:link href="/categories/meetups/index.xml" rel="self" type="application/rss+xml" />
    <description>meetups</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2019</copyright><lastBuildDate>Tue, 07 Feb 2017 16:28:27 +0800</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>meetups</title>
      <link>/categories/meetups/</link>
    </image>
    
    <item>
      <title>The AI scene in the valley</title>
      <link>/post/ai-trip/</link>
      <pubDate>Tue, 07 Feb 2017 16:28:27 +0800</pubDate>
      <guid>/post/ai-trip/</guid>
      <description>&lt;p&gt;A few weeks back I was lucky enough to attend and present at the &lt;a href=&#34;http://globalbigdataconference.com/santa-clara/global-artificial-intelligence-conference/schedule-83.html&#34;&gt;Global AI Summit&lt;/a&gt; in the bay area. This is my personal trip report about the people I have met and some of the projects and startups I came across.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI is eating the world.&lt;/strong&gt;&lt;br&gt;
First of all, let me start by saying that literally &lt;em&gt;everybody&lt;/em&gt; is doing (or claiming to do) AI in the bay area. AI has inflamed the spirits of pretty much every single software engineer, data scientist, business developer, talent scout, and VC in the greater San Francisco area.&lt;/p&gt;
&lt;p&gt;All tools and services presented at the conference embed some form of machine intelligence, and scientists are the new cool kids on the block. Software engineering has probably reached an all-time low in terms of coolness in the bay area, and regarded almost as the &amp;ldquo;necessary evil&amp;rdquo; in order to unleash the next AI interface. This is somewhat counter-intuitive, as actually &lt;a href=&#34;http://www.kdnuggets.com/2016/03/dont-buy-machine-learning.html&#34;&gt;Machine Learning and AI are more like the raisins in raisin bread&lt;/a&gt;, as Peter Norvig and &lt;a href=&#34;http://twitter.com/marcossponton&#34;&gt;Marcos Sponton&lt;/a&gt; say.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&#39;s behind AI?&lt;/strong&gt;&lt;br&gt;
Good engineering, and great business focus is still the foundation of many AI-powered tools and services. In my opinion, there is no silver bullet: AI powered applications must still be based on good engineering practices if they want to succeed.&lt;/p&gt;
&lt;p&gt;We have seen a similar wave during the &lt;a href=&#34;http://en.wikipedia.org/wiki/Dot-com_bubble&#34;&gt;dot-com&lt;/a&gt; bubble in the beginning of the millennium when web-shops were popping up with little understanding of the underlying retail and marketplace businesses. Since then, web applications have matured and today we value those services both for their digital journey as much as for their operational excellence and their ability to deliver. I believe that a similar maturing path will happen for AI powered applications.&lt;/p&gt;
&lt;p&gt;AI is still a very opaque concept. In the worst case it could be just a scripted process, more often is a set of predictive machine learning models. Because of the vagueness of the term, others are branding new terms in order to differentiate themselves: machine intelligence, cognitive/sentient computing, intelligent computing. Advertising more AI-related terms is not really helping clarifying what is running under the hood. After some digging, startups operating in the AI space are mainly interpreting AI as some form of artificial/machine learning (aka &lt;a href=&#34;http://en.wikipedia.org/wiki/Weak_AI&#34;&gt;weak AI&lt;/a&gt;) tailored to very specific tasks.&lt;/p&gt;
&lt;p&gt;Today, with some exceptions, the term AI is used to describe &lt;a href=&#34;http://en.wikipedia.org/wiki/Artificial_neural_network&#34;&gt;Artificial Neural Networks&lt;/a&gt; (ANNs), and &lt;a href=&#34;http://en.wikipedia.org/wiki/Deep_learning&#34;&gt;Deep Learning&lt;/a&gt; (DL) mostly related to text, speech, image, and video processing. Putting the hype aside for a moment, without any doubts we can acknowledge that &lt;a href=&#34;http://lecture2go.uni-hamburg.de/l2go/-/get/v/16622&#34;&gt;the renaissance of deep learning&lt;/a&gt; has contributed to the development of conversational interfaces.&lt;/p&gt;
&lt;p&gt;The core of this new generation services might by still hard-coded or scripted, but the interface is going to be more and more flexible, understanding our spoken, text, and visual cues. This human-centric approach to UIs is definitely going to shape the way we interact with devices. This trend goes under the buzz of &lt;a href=&#34;http://blog.careerfoundry.com/ui-design/what-is-zero-ui&#34;&gt;Natural/Zero UIs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&#39;s go deeper in the stack, away from the front-end and human-machine Natural UIs. &lt;a href=&#34;http://en.wikipedia.org/wiki/Weak_AI&#34;&gt;Narrow AI&lt;/a&gt;, in particular deep learning and hierarchical predictive models are getting traction as core data components, in particular for applications such as &lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf&#34;&gt;recommender systems&lt;/a&gt;, fraud detection, churn and propensity models, anomaly detection and data auditing.&lt;/p&gt;
&lt;p&gt;Before moving on the following list: I am not associated with any of these companies, however I did find their approach worth mentioning and good food for thoughts for the entrepreneurs and the data people following this blog. So, as always, take the following with a pinch of salt and apply your critical &amp;amp; analytical thinking to it. Enjoy :)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://numenta.com/applications/&#34;&gt;Numenta&lt;/a&gt; is tackling one of the most important scientific challenges of all time: reverse engineering the neocortex. Studying how the brain works helps us understand the principles of intelligence and build machines that work on the same principles. They have invested heavily in time series research, anomaly detection and natural language processing. By converting text, and geo-spatial data to time series Numenta can detect patterns and anomalies in temporal data.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.recognos.com/smart-data-platform/&#34;&gt;Recognos&lt;/a&gt;&amp;rsquo; main product &amp;ldquo;Smart Data Platform&amp;rdquo; is meant to normalize and integrate the data that is stored in unstructured, semi-structured and structured content. The data unification process is driven by the business ontology. Data extraction, taxonomy, semantic tagging and structured data mapping are all steps in this modern approach to data preparation and normalization. &lt;a href=&#34;http://www.recognos.com/smart-data-platform/&#34;&gt;Recognos&lt;/a&gt;&amp;rsquo; ultimate goal is to allow to use the data that is stored in the un-structured, semi-structured and structured content using a unique semantic meaning and unique query language.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.appzen.com/&#34;&gt;Appzen&lt;/a&gt; is picking up the challenge of automating the auditing of expenses in real-time. This service collects all receipts, tickets, and other documentation provided and produces a full understanding of the who, where, and why of every expense. &lt;a href=&#34;http://www.appzen.com/&#34;&gt;Appzen&lt;/a&gt;&amp;lsquo;s machine learning engine verifies receipts, eliminates duplicates, and searches through hundreds of data sources to verify merchant identities and validate the truthfulness of every expense – ensuring there is no fraud, misuse, or violation of laws.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://talla.com/features&#34;&gt;Talla&lt;/a&gt; is a lightweight system which plugs into messaging systems as a virtual team members, executing on-boarding, e-learning, and team building process flows, and engaging with the various team members, taking over from tasks which are usually done by team managers, scrum masters, and facilitating teams. It employs a combination of natural language processing, robotic process automation, and user- and company- defined rules.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.inbenta.com/en/features/self-service/&#34;&gt;Inbenta&lt;/a&gt; has build an extensive multilingual knowledge supporting over 25 native languages and counting, including English, Spanish, Italian, Dutch and German. Its NLP engine understands the nuances of human conversation, so it answers a query based on meaning, not individual keywords. This is a good example of a company which relies on good business development and a great core team of linguistics and language experts. By combining these elements with NLP and Deep Learning techniques they can power &lt;a href=&#34;http://medium.com/cyber-tales/ai-and-speech-recognition-a-primer-for-chatbots-a63af042526a#.o8svmik8j&#34;&gt;chatbots&lt;/a&gt;, email management, surveys and other text-based use cases for a number of verticals.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lymba.com/knowledge-extraction-k-extractor.html&#34;&gt;Lymba&lt;/a&gt; is also tackling textual information, with the goal of extracting insights and non-trivial bits of knowledge from a semantic graph of heterogeneous linked data elements. Lymba offers transformative capabilities that enable discovery of actionable knowledge from structured and unstructured data, providing business with invaluable insights. Lymba has developed a technology for deep semantic processing that extracts semantic relations between all concepts in the text. One of Lymba&#39;s product, the &amp;ldquo;K-extractor&amp;rdquo; enables intelligent search, question answering, summarization of a document, generating scientific profiles, etc.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.jetlore.com/dynamic-layouts/&#34;&gt;Jetlore&lt;/a&gt; is bringing personalization one step further by creating websites and mobile apps which are extremely tailored to each user, both in terms of content as well as layout, color, highlights, promotions, images, and offers. All site assets are ranked individually for each customer, and selected at the time of interaction based on the layout&#39;s configuration. Jetlore can select the best categories, brands, and collections of products from your inventory for each user, and automatically feature the best images to represent them. &lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://ownerlistens.com/&#34;&gt;Ownerlisten&lt;/a&gt; is a smart messaging pipelining solution, rerouting messages in a given organization to the right person or process depending on the nature of the message. Users can filter and process messages combining business rules as well as automated text processing. This is essential in businesses where machine learned models might not be provide sufficiently accuracy for certain topics. &lt;a href=&#34;http://ownerlistens.com/&#34;&gt;Ownerlisten&lt;/a&gt; is another good example of how AI and NLP can be organically combined with user defined messaging and communication flows. By combining domain expertise, solid engineering and NLP engines, &lt;a href=&#34;http://ownerlistens.com/&#34;&gt;ownerlisten&lt;/a&gt; can deliver very smooth user and customer journeys in a number of different industries and use cases.&lt;/p&gt;
&lt;p&gt;This list would not be complete without at least a company offering AI, Machine Learning, Data plumbing, Data engineering, API and Application engineering services. Software and data engineering, might be less cool a land of scientists, but it&#39;s still the backbone on top of which all those awesome solutions and products are built. &lt;a href=&#34;http://www.datamonsters.co/&#34;&gt;Data Monster&lt;/a&gt; is one of those great studios accelerating mvp and product development, with a strong affinity to data processing at scale and all the right techs in the basket (Scala, Python, R, Java, JavaScript, Hadoop, Spark, Hive, Play, Akka, MySQL, PostgreSQL, AWS, Cassandra, etc ).&lt;/p&gt;
&lt;p&gt;I finish this post mentioning and thanking a number of great people I have met during this trip, for their charisma and inspiring ideas and conversations:
&lt;a href=&#34;http://www.linkedin.com/in/hamid-pirahesh-38368010/&#34;&gt;Hamid Pirahesh&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/davidtalby/&#34;&gt;David Talby&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/chiefscientist/&#34;&gt;Alexy Khabrov&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/atsyplikhin/&#34;&gt;Alexander Tsyplikhin&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/chrisemoody/&#34;&gt;Christopher Moody&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/mifeng/&#34;&gt;Michael Feng&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/deliprao/&#34;&gt;Delip Rao&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/eldarsadikov/&#34;&gt;Eldar Sadikov&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/michellecasbon/&#34;&gt;Michelle Casbon&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/mustafameisa/&#34;&gt;Mustafa Eisa&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/ahmed-bakhaty/&#34;&gt;Ahmed Bakhaty&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/adibittan/&#34;&gt;Adi Bittan&lt;/a&gt;,
&lt;a href=&#34;http://www.linkedin.com/in/jtorras/&#34;&gt;Jordi Torras&lt;/a&gt;, and
&lt;a href=&#34;http://www.linkedin.com/in/francesco-corea-6b4b4a44/&#34;&gt;Francesco Corea&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science Q&amp;A</title>
      <link>/post/data-science-qa/</link>
      <pubDate>Tue, 07 Feb 2017 00:00:00 +0000</pubDate>
      <guid>/post/data-science-qa/</guid>
      <description>&lt;p&gt;I was kindly asked by Prof. Roberto Zicari to answer a few questions on Data Science and Big Data for &lt;a href=&#34;http://www.odbms.org&#34;&gt;www.odbms.org&lt;/a&gt; - Let me know what you think of it, looking forward to your feedback in the comment below. Cheers, Natalino&lt;/p&gt;
&lt;p&gt;Q1. Is domain knowledge necessary for a data scientist?&lt;/p&gt;
&lt;p&gt;It’s not strictly necessary, but it does not harm either. You can produce accurate models without having to understand the domain. However, some domain knowledge will speed up the process of selecting relevant features and will provide a better context for knowledge discovery in the available datasets.&lt;/p&gt;
&lt;p&gt;Q2. What should every data scientist know about machine learning?&lt;/p&gt;
&lt;p&gt;First of all, the foundation: statistics, algebra and calculus. Vector, matrix and tensor math is absolutely a must. Let’s not forget that datasets after all can be handled as large matrices! Moving on specifically on the topic of machine learning: a good understanding of the role of &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34;&gt;bias and variance&lt;/a&gt; for predictive models. Understanding the reasons for models and parameters &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_(mathematics)&#34;&gt;regularization&lt;/a&gt;. Model &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&#34;&gt;cross-validation&lt;/a&gt; techniques. Data &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&#34;&gt;bootstrapping&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrap_aggregating&#34;&gt;bagging&lt;/a&gt;. Also I believe that cost based, gradient iterative optimization methods are a must, as they implement the “learning” for four very powerful classes of machine learning algorithms: glm’s, boosted trees, svm and kernel methods, neural networks. Last but not least an introduction to Bayesian statistics as many&lt;/p&gt;
&lt;p&gt;Q3. What are the most effective machine learning algorithms?&lt;/p&gt;
&lt;p&gt;Regularized &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_linear_model&#34;&gt;Generalized Linear Models&lt;/a&gt;, and their further generalization as Artificial Neural Networks (&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34;&gt;ANN’s&lt;/a&gt;), &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;Boosted&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;Random Forests&lt;/a&gt;. Also I am very interested in dimensionality reduction and unsupervised machine learning algorithms, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding&#34;&gt;T-SNE&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/OPTICS_algorithm&#34;&gt;OPTICS&lt;/a&gt;, and &lt;a href=&#34;https://en.wikipedia.org/wiki/Topological_data_analysis&#34;&gt;TDA&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Q4. What is your experience with data blending?&lt;/p&gt;
&lt;p&gt;Blending data from different domains and sources might increases the explanatory power of the model. However, it’s not always easy to determine beforehand if this data will improve the models. Data blending provide more features and the may or may be not correlated with what you wish to predict. It’s therefore very important to carefully validate the trained model using cross validation and other statistical methods such as variances analysis on the augmented dataset.&lt;/p&gt;
&lt;p&gt;Q5. Predictive Modeling: How can you perform accurate feature engineering/extraction?&lt;/p&gt;
&lt;p&gt;Let’s tackle feature extraction and feature engineering separately. Extraction can be as simple as getting a number of fields from a database table, and as complicated as extracting information from a scanned paper document using &lt;a href=&#34;https://en.wikipedia.org/wiki/Optical_character_recognition&#34;&gt;OCR&lt;/a&gt; and image processing techniques. Feature extraction can easily be the hardest task in a given data science engagement.&lt;/p&gt;
&lt;p&gt;Extracting the right features and raw data fields usually requires a good understanding of the organization, the processes and the physical/digital data building blocks deployed in a given enterprise. It’s a task which should never be underestimated as usually the predictive model is just as good as the data which is used to train it.&lt;/p&gt;
&lt;p&gt;After extraction, there comes feature engineering. This step consists of a number of data transformations, oftentimes dictated by a combination of intuition, data exploration, and domain knowledge. Engineered features are usually added to the original samples’ features and provided as the input data to the model.&lt;/p&gt;
&lt;p&gt;Before the renaissance of neural networks and hierarchical machine learning, feature engineering was as the models were too shallow to properly transform the input data in the model itself. For instance, &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree&#34;&gt;decision trees&lt;/a&gt; can only split data areas along the features’ axes, therefore to correctly classify donut shaped classes you will need feature engineering to transform the space to polar coordinates.&lt;/p&gt;
&lt;p&gt;In the past years, however, models usually have multiple layers, as machine learning experts are deploying increasingly “deeper” models. Those models usually can “embed” feature engineering as part of the internal state representation of data, rendering manual feature engineering less relevant. For some examples applied to text check the section “Visualizing the predictions and the “neuron” firings in the RNN” in &lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;. These models are also usually referred as “end-to-end” learning, although this definition it’s still vague not unanimously accepted in the AI and Data Science communities.&lt;/p&gt;
&lt;p&gt;So what about feature engineering today? Personally, I do believe that some feature engineering is still relevant to build good predictive systems, but should not be overdone, as many features can be now learned by the model itself, especially in the audio, video, text, speech domains.&lt;/p&gt;
&lt;p&gt;Q6. Can data ingestion be automated?&lt;/p&gt;
&lt;p&gt;Yes. But beware of metadata management. In particular, I am a big supporter of “closed loop” analytics on metadata, where changes in the data source formats or semantics are detected by means of analytics and machine learning on the metadata itself.&lt;/p&gt;
&lt;p&gt;Q7. How do you ensure data quality?&lt;/p&gt;
&lt;p&gt;I tend to rely on the “wisdom of the crowd” by implementing similar analyses using multiple techniques and machine learning algorithms. When the results diverge, I compare the methods to gain any insight about the quality of both data as well as models. This approach works also well to validate the quality of streaming analytics: in this case the batch historical data can be used to double check the result in streaming mode, providing, for instance, end-of-day or end-of-month reporting for data correction and reconciliation.&lt;/p&gt;
&lt;p&gt;Q8. What techniques can you use to leverage interesting metadata?&lt;/p&gt;
&lt;p&gt;Fingerprinting is definitely an interesting field for metadata generation. I have worked extensively in the past on audio and video fingerprinting. However this technique is very general and can be applied to any sort of data: structure data, time series, etc. Data fingerprinting can be used to summarize web pages retrieved by users or to define the nature and the characteristics of data flows in network traffic. I also work often with time (event time, stream time, capture time), network data (ip/mac addresses, payloads, etc.) and geolocated information to produce rich metadata for my data science projects and tutorials.&lt;/p&gt;
&lt;p&gt;Q9. How do you evaluate if the insight you obtain from data analytics is &amp;ldquo;correct&amp;rdquo; or &amp;ldquo;good&amp;rdquo; or &amp;ldquo;relevant&amp;rdquo; to the problem domain?&lt;/p&gt;
&lt;p&gt;Initially, I interact with domain experts for a first review on the results. Subsequently, I make sure than the model is brought into “action”. Relevant insight, in my opinion, can always be assessed by measuring their positive impact on the overall application. If human interaction is in the loop, the easiest method is actually to measure the impact of the relevant insight in their digital journey.&lt;/p&gt;
&lt;p&gt;Q10. What were your most successful data projects? And why?&lt;/p&gt;
&lt;p&gt;1. Geolocated data pattern analysis, because of its application to fraud prevention and personalized recommendations. 2. time series analytics for anomaly detection and forecasting of temporal signals - in particular for enterprise processes and KPI’s. 3. Converting images to features, because it allows images/videos to be indexed and classified using standard BI tools.&lt;/p&gt;
&lt;p&gt;Q11. What are the typical mistakes done when analyzing data for a large scale data project? Can they be avoided in practice?&lt;/p&gt;
&lt;p&gt;Aggregating too much will most of the time “flatten” signals in large datasets. To prevent this, try using more features, and/or provide a finer segmentation of the data space. Another common problem is “buring” signals provided by a small class of samples with those of a dominating class. Models discriminating unbalanced classes tend to perform worse as the dataset grows. To solve this problem try to rebalance the classes by applying stratified resampling, or weighting the results, or boosting on the weak signals.&lt;/p&gt;
&lt;p&gt;Q12. What are the ethical issues that a data scientist must always consider?&lt;/p&gt;
&lt;p&gt;1. Respect individual privacy and possibly enforce it algorithmically. 2. Be transparent and fair on the use of the provided data with the legal entities and the individuals who have generated the data. 3. Avoid building models discriminating and scoring based on race, religion, sex, age etc as much as possible and be aware of the implication of reinforcing decisions based on the available data labels.&lt;/p&gt;
&lt;p&gt;On last point, I would like to close this interview with an interesting idea around “equal opportunity” for ethical machine learning. This concept is visually explained on the following Google Research page &lt;a href=&#34;https://research.google.com/bigpicture/attacking-discrimination-in-ml/&#34;&gt;Attacking discrimination with smarter machine learning&lt;/a&gt; from a recent paper by &lt;a href=&#34;https://drive.google.com/file/d/0B-wQVEjH9yuhanpyQjUwQS1JOTQ/view&#34;&gt;Hardt, Price, Srebro&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
